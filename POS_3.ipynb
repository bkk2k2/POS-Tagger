{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POS-3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uf1O_IHmhHQ",
        "outputId": "bfb18b43-2c10-4c45-d0e8-3a6a3016a85f"
      },
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import treebank, brown, conll2000\n",
        "import tensorflow as tf\n",
        "nltk.download(\"treebank\")\n",
        "nltk.download(\"brown\")\n",
        "nltk.download(\"conll2000\")\n",
        "nltk.download(\"universal_tagset\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQ8TQQxYmQYQ"
      },
      "source": [
        "treebank_corpus = treebank.tagged_sents(tagset='universal')\n",
        "brown_corpus = brown.tagged_sents(tagset='universal')\n",
        "conll_corpus = conll2000.tagged_sents(tagset='universal')\n",
        "\n",
        "sentences = treebank_corpus + brown_corpus + conll_corpus\n",
        "#sentences = treebank.tagged_sents()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q24hwkM0macm"
      },
      "source": [
        "X = [] # store input sequence\n",
        "Y = [] # store output sequence\n",
        "for sentence in sentences:\n",
        "  X_sentence = []\n",
        "  Y_sentence = []\n",
        "  for (word, tag) in sentence: \n",
        "    X_sentence.append(word) # entity[0] contains the word\n",
        "    Y_sentence.append(tag) # entity[1] contains corresponding tag\n",
        " \n",
        "  X.append(X_sentence)\n",
        "  Y.append(Y_sentence)\n",
        "num_words = len(set([word.lower() for sentence in X for word in sentence]))\n",
        "num_tags = len(set([word.lower() for sentence in Y for word in sentence]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOv-Eq1jmyLh"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# encode X\n",
        "word_tokenizer = Tokenizer()              # instantiate tokeniser\n",
        "word_tokenizer.fit_on_texts(X)            # fit tokeniser on data\n",
        "# use the tokeniser to encode input sequence\n",
        "X_encoded = word_tokenizer.texts_to_sequences(X)  \n",
        "# encode Y\n",
        "tag_tokenizer = Tokenizer()\n",
        "tag_tokenizer.fit_on_texts(Y)\n",
        "Y_encoded = tag_tokenizer.texts_to_sequences(Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2IBLVolnCuj"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "MAX_SEQ_LENGTH = 100\n",
        "X_padded = pad_sequences(X_encoded, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')\n",
        "Y_padded = pad_sequences(Y_encoded, maxlen=MAX_SEQ_LENGTH, padding='post', truncating='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzTLJKL8nS3d"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "Y_cat = to_categorical(Y_padded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcRLuuIyoCKb"
      },
      "source": [
        "from keras import backend as K\n",
        " \n",
        "def ignore_class_accuracy(to_ignore=0):\n",
        "    def ignore_accuracy(y_true, y_pred):\n",
        "        y_true_class = K.argmax(y_true, axis=-1)\n",
        "        y_pred_class = K.argmax(y_pred, axis=-1)\n",
        " \n",
        "        ignore_mask = K.cast(K.not_equal(y_pred_class, to_ignore), 'int32')\n",
        "        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n",
        "        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n",
        "        return accuracy\n",
        "    return ignore_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvrLdDJjoE2d"
      },
      "source": [
        "embeddings_index = {}\n",
        "GLOVE_PATH = \"/content/drive/MyDrive/MZ/Glove embeddings/glove.6B.300d.txt\"\n",
        "f = open(GLOVE_PATH)\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PE07P2GYLmFa"
      },
      "source": [
        "embedding_dim = 300\n",
        "word_index = word_tokenizer.word_index\n",
        "# first create a matrix of zeros, this is our embedding matrix\n",
        "embedding_matrix = np.zeros((num_words+2, embedding_dim))\n",
        "\n",
        "# for each word in out tokenizer lets try to find that work in our w2v model\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # we found the word - add that words vector to the matrix\n",
        "        embedding_matrix[i+1] = embedding_vector\n",
        "    else:\n",
        "        # doesn't exist, assign a random vector\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9DFgqpoMRgF",
        "outputId": "0be85156-286e-4930-d13b-f94b17e71ddd"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "  tf.keras.layers.Input(shape=(100,)),\n",
        "  tf.keras.layers.Embedding(input_dim=num_words+2, output_dim=300, input_length=100,weights = [embedding_matrix],trainable=True),\n",
        "  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True), input_shape=(100,9)),\n",
        "  tf.keras.layers.Dense(num_tags+1, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['acc',ignore_class_accuracy(0)])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_6 (Embedding)      (None, 100, 300)          17835000  \n",
            "_________________________________________________________________\n",
            "bidirectional_6 (Bidirection (None, 100, 128)          186880    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100, 13)           1677      \n",
            "=================================================================\n",
            "Total params: 18,023,557\n",
            "Trainable params: 18,023,557\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mYXQhPylNEA7",
        "outputId": "b8668c89-accb-4a85-d4ca-5087ec2d10f9"
      },
      "source": [
        "history = model.fit(np.array(X_padded), np.array(Y_cat), epochs=5, batch_size=128, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "452/452 [==============================] - 339s 742ms/step - loss: 0.4908 - acc: 0.9073 - ignore_accuracy: 0.6312 - val_loss: 0.0567 - val_acc: 0.9817 - val_ignore_accuracy: 0.9190\n",
            "Epoch 2/5\n",
            "452/452 [==============================] - 334s 738ms/step - loss: 0.0270 - acc: 0.9919 - ignore_accuracy: 0.9615 - val_loss: 0.0457 - val_acc: 0.9847 - val_ignore_accuracy: 0.9326\n",
            "Epoch 3/5\n",
            "452/452 [==============================] - 340s 753ms/step - loss: 0.0156 - acc: 0.9952 - ignore_accuracy: 0.9769 - val_loss: 0.0436 - val_acc: 0.9856 - val_ignore_accuracy: 0.9369\n",
            "Epoch 4/5\n",
            "452/452 [==============================] - 334s 740ms/step - loss: 0.0116 - acc: 0.9964 - ignore_accuracy: 0.9825 - val_loss: 0.0448 - val_acc: 0.9857 - val_ignore_accuracy: 0.9373\n",
            "Epoch 5/5\n",
            "452/452 [==============================] - 331s 732ms/step - loss: 0.0094 - acc: 0.9970 - ignore_accuracy: 0.9856 - val_loss: 0.0458 - val_acc: 0.9860 - val_ignore_accuracy: 0.9388\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9gdSkQTNKmp"
      },
      "source": [
        "def logits_to_tokens(sequences):\n",
        "    token_sequences = []\n",
        "    for categorical_sequence in sequences:\n",
        "        token_sequence = []\n",
        "        for categorical in categorical_sequence:\n",
        "            arg = np.argmax(categorical)\n",
        "            token_sequence.append(tag_tokenizer.sequences_to_texts([[arg]]))\n",
        " \n",
        "        token_sequences.append(token_sequence)\n",
        " \n",
        "    return token_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXlcHN8DNQVI",
        "outputId": "b6a2f82c-aad4-4536-dc48-46771d72e3f2"
      },
      "source": [
        "test_sent = \"I am a boy .\"\n",
        "test = [test_sent.split()]\n",
        "test_input = word_tokenizer.texts_to_sequences(test)\n",
        "test_input_padded = pad_sequences(test_input, maxlen=MAX_SEQ_LENGTH, padding=\"post\", truncating=\"post\")\n",
        "predictions = model.predict(test_input_padded)\n",
        "print(logits_to_tokens(predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[['pron'], ['verb'], ['det'], ['noun'], ['.'], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], [''], ['']]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoDqIkRqXWAV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}